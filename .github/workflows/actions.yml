name: Automation Framework CI with Xray Integration

on:
  push:
    branches: [ "**" ]  # Trigger on push to any branch

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:latest
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'  # Adjust based on your Python version
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # First install a compatible version of pytest for Xray
        pip install pytest==7.4.0
        # Then install pytest-xray
        pip install pytest-xray==0.2.1
        # Check if pytest-xray is correctly installed
        python -c "import pytest_xray; print('pytest-xray version:', pytest_xray.__version__)"
        pip list | grep pytest
        
        # Now install the rest
        pip install -e .
        # Use --no-deps to avoid overwriting pytest version
        pip install --no-deps -r requirements.txt
        # Manually install other dependencies to avoid conflicts
        pip install allure-pytest playwright pytest-playwright pytest-xdist requests python-dotenv psycopg2-binary clickhouse-driver boto3 botocore openai

    - name: Install Playwright browsers
      run: |
        python -m playwright install --with-deps

    - name: Install Allure CLI
      run: |
        curl -o allure-2.24.1.tgz -OLs https://repo.maven.apache.org/maven2/io/qameta/allure/allure-commandline/2.24.1/allure-commandline-2.24.1.tgz
        tar -zxvf allure-2.24.1.tgz -C /tmp
        sudo ln -s /tmp/allure-2.24.1/bin/allure /usr/local/bin/allure
        allure --version

    - name: Verify Xray Configuration
      run: |
        # Create a simple test to check Xray configuration
        mkdir -p test_xray
        cat > test_xray/test_basic.py << EOF
        import pytest
        
        @pytest.mark.xray("TEST-123")
        def test_basic():
            assert True
        EOF
        
        # Configure Xray for pytest 7.x
        cat > xray_config.ini << EOF
        [pytest]
        xray_enabled = true
        xray_base_url = "${JIRA_URL}"
        xray_username = "${JIRA_USERNAME}"
        xray_password = "${JIRA_PASSWORD}"
        xray_client_id = "${XRAY_CLIENT_ID}"
        xray_client_secret = "${XRAY_CLIENT_SECRET}"
        xray_test_plan_key = "${XRAY_TESTPLAN_KEY}"
        xray_output_path = "reports/xray-results/xray-report.json"
        EOF
        
        mkdir -p reports/xray-results
        
        # Run this one test with the xray config
        PYTHONDONTWRITEBYTECODE=1 python -m pytest test_xray -v -c xray_config.ini
      env:
        JIRA_URL: ${{ secrets.JIRA_URL }}
        JIRA_USERNAME: ${{ secrets.JIRA_USERNAME }}
        JIRA_PASSWORD: ${{ secrets.JIRA_API_TOKEN }}
        XRAY_CLIENT_ID: ${{ secrets.XRAY_CLIENT_ID }}
        XRAY_CLIENT_SECRET: ${{ secrets.XRAY_CLIENT_SECRET }}
        XRAY_TESTPLAN_KEY: ${{ github.event.inputs.testplan_key || '' }}
        
    - name: Run tests
      run: |
        mkdir -p reports/allure-results
        
        # GitHub Actions runners typically have 2 cores
        # Disable assertion rewriting to avoid the lineno error
        PYTHONDONTWRITEBYTECODE=1 python -m pytest tests -v -n 2 --alluredir=reports/allure-results --dist=loadfile -c xray_config.ini
      env:
        POSTGRES_HOST: localhost
        POSTGRES_PORT: 5432
        POSTGRES_USER: postgres
        POSTGRES_PASSWORD: postgres
        POSTGRES_DB: testdb
        JIRA_URL: ${{ secrets.JIRA_URL }}
        JIRA_USERNAME: ${{ secrets.JIRA_USERNAME }}
        JIRA_PASSWORD: ${{ secrets.JIRA_API_TOKEN }}
        XRAY_CLIENT_ID: ${{ secrets.XRAY_CLIENT_ID }}
        XRAY_CLIENT_SECRET: ${{ secrets.XRAY_CLIENT_SECRET }}
        XRAY_TESTPLAN_KEY: ${{ github.event.inputs.testplan_key || '' }}

    - name: Generate Xray Report
      if: always()  # Generate report even if tests fail
      run: |
        # Create a direct Xray report based on pytest test results
        python - << 'EOF'
        import json
        import glob
        import re
        import os
        import xml.etree.ElementTree as ET
        from datetime import datetime
        
        # Create the Xray report structure
        xray_report = {
            "info": {
                "summary": f"Test Execution - GitHub CI - {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                "description": "Automated test execution from GitHub Actions workflow",
                "testEnvironments": ["GitHub CI"]
            },
            "tests": []
        }
        
        # First, generate a JUnit XML report from the test results
        os.system('python -m pytest --collect-only tests -v --junitxml=reports/pytest_results.xml')
        
        # Check if the JUnit XML report was generated
        if os.path.exists('reports/pytest_results.xml'):
            try:
                # Parse the JUnit XML report
                tree = ET.parse('reports/pytest_results.xml')
                root = tree.getroot()
                
                # Process test cases
                for testcase in root.findall('.//testcase'):
                    test_name = testcase.get('name')
                    test_class = testcase.get('classname')
                    full_name = f"{test_class}::{test_name}"
                    
                    # Try to extract a Jira ID from the test name or class
                    jira_id = None
                    jira_match = re.search(r'([A-Z]+-\d+)', full_name)
                    if jira_match:
                        jira_id = jira_match.group(1)
                    
                    # If no Jira ID in the name, try to inspect the source code
                    if not jira_id:
                        # This is a simple approach - you might need to enhance it
                        module_path = test_class.replace('.', '/') + '.py'
                        if os.path.exists(module_path):
                            with open(module_path, 'r') as f:
                                source = f.read()
                                # Look for @pytest.mark.xray pattern
                                mark_match = re.search(r'@pytest\.mark\.xray\(["\']([A-Z]+-\d+)["\']\)', source)
                                if mark_match:
                                    jira_id = mark_match.group(1)
                    
                    # Determine test status
                    status = "PASS"
                    for child in testcase:
                        if child.tag in ['failure', 'error', 'skipped']:
                            status = "FAIL" if child.tag in ['failure', 'error'] else "SKIP"
                            break
                    
                    # Only add tests with a Jira ID
                    if jira_id:
                        xray_report['tests'].append({
                            "testKey": jira_id,
                            "status": status,
                            "comment": f"Execution from GitHub CI: {full_name}"
                        })
                    else:
                        print(f"Warning: No Jira ID found for test {full_name}")
                
                # Save the Xray report
                os.makedirs('reports/xray-results', exist_ok=True)
                with open('reports/xray-results/xray-report.json', 'w') as f:
                    json.dump(xray_report, f, indent=2)
                
                print(f"Generated Xray report with {len(xray_report['tests'])} tests")
                
            except Exception as e:
                print(f"Error processing test results: {str(e)}")
        else:
            print("No JUnit XML report found")
            # Create a minimal report to avoid errors in subsequent steps
            os.makedirs('reports/xray-results', exist_ok=True)
            with open('reports/xray-results/xray-report.json', 'w') as f:
                json.dump(xray_report, f, indent=2)
        EOF

    - name: Generate Allure report
      if: always()  # Generate report even if tests fail
      run: |
        allure generate reports/allure-results -o reports/allure-report --clean

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: allure-report
        path: reports/allure-report/
        retention-days: 30

    - name: Deploy Allure report to GitHub Pages
      if: github.ref == 'refs/heads/main' && always()  # Only deploy from main branch
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./reports/allure-report
        publish_branch: gh-pages
        
    - name: Upload Xray Test Results to Jira
      if: always()  # Upload results even if tests fail
      run: |
        if [ -f "reports/xray-results/xray-report.json" ]; then
          echo "Uploading Xray test results..."
          echo "Content of xray-report.json:"
          cat reports/xray-results/xray-report.json
          
          # Try different Xray API endpoints one by one
          
          # 1. Xray Cloud Import Execution endpoint
          echo "\nTrying Xray Cloud Import Execution endpoint..."
          if curl -s -o xray_response.json -w "%{http_code}" \
               -H "Content-Type: application/json" \
               -X POST \
               -u "${{ secrets.JIRA_USERNAME }}:${{ secrets.JIRA_API_TOKEN }}" \
               --data @reports/xray-results/xray-report.json \
               "${{ secrets.JIRA_URL }}/rest/raven/1.0/import/execution" | grep -q "2[0-9][0-9]"; then
            echo "Successfully uploaded test results to Xray Cloud"
            cat xray_response.json
            exit 0
          else
            echo "Failed with this endpoint. Response:"
            cat xray_response.json || echo "No response"
          fi
          
          # 2. Xray Server Import Execution endpoint
          echo "\nTrying Xray Server Import Execution endpoint..."
          if curl -s -o xray_response.json -w "%{http_code}" \
               -H "Content-Type: application/json" \
               -X POST \
               -u "${{ secrets.JIRA_USERNAME }}:${{ secrets.JIRA_API_TOKEN }}" \
               --data @reports/xray-results/xray-report.json \
               "${{ secrets.JIRA_URL }}/rest/raven/2.0/import/execution" | grep -q "2[0-9][0-9]"; then
            echo "Successfully uploaded test results to Xray Server"
            cat xray_response.json
            exit 0
          else
            echo "Failed with this endpoint. Response:"
            cat xray_response.json || echo "No response"
          fi

          # 3. Try a custom JSON format for Xray server format
          echo "\nReformatting for Xray Server and trying again..."
          
          # Convert our report format to Xray Server format
          python - << 'EOF'
          import json
          import os
          
          try:
              with open('reports/xray-results/xray-report.json', 'r') as f:
                  report = json.load(f)
              
              # Extract test data
              test_info = report.get('info', {})
              tests = report.get('tests', [])
              
              # Create Xray Server format (different from Cloud)
              server_format = {
                  "testExecutionKey": "",  # Will be created by Xray
                  "info": {
                      "summary": test_info.get('summary', 'Test Execution from GitHub CI'),
                      "description": test_info.get('description', 'Automated test execution')
                  },
                  "tests": []
              }
              
              # Convert test entries
              for test in tests:
                  server_format["tests"].append({
                      "testKey": test.get("testKey", ""),
                      "status": test.get("status", "FAIL"),
                      "comment": test.get("comment", "")
                  })
              
              # Write the reformatted report
              with open('reports/xray-results/xray-server-report.json', 'w') as f:
                  json.dump(server_format, f, indent=2)
              
              print("Successfully reformatted the report for Xray Server")
          except Exception as e:
              print(f"Error reformatting report: {str(e)}")
          EOF
          
          if [ -f "reports/xray-results/xray-server-report.json" ]; then
            echo "Content of xray-server-report.json:"
            cat reports/xray-results/xray-server-report.json
            
            echo "\nTrying Xray Server with reformatted report..."
            if curl -s -o xray_response.json -w "%{http_code}" \
                 -H "Content-Type: application/json" \
                 -X POST \
                 -u "${{ secrets.JIRA_USERNAME }}:${{ secrets.JIRA_API_TOKEN }}" \
                 --data @reports/xray-results/xray-server-report.json \
                 "${{ secrets.JIRA_URL }}/rest/raven/1.0/import/execution" | grep -q "2[0-9][0-9]"; then
              echo "Successfully uploaded reformatted test results to Xray"
              cat xray_response.json
              exit 0
            else
              echo "Failed with reformatted report. Response:"
              cat xray_response.json || echo "No response"
            fi
          fi
          
          # 4. Create test execution directly via Jira API
          echo "\nAttempting direct test execution creation via Jira API..."
          
          # Convert to direct Jira issue creation format
          python - << 'EOF'
          import json
          import os
          import uuid
          
          try:
              with open('reports/xray-results/xray-report.json', 'r') as f:
                  report = json.load(f)
              
              # Extract test data
              test_info = report.get('info', {})
              tests = report.get('tests', [])
              
              if len(tests) == 0:
                  print("No tests to report")
                  exit(0)
              
              # Get a test key to determine project
              first_test = tests[0].get("testKey", "")
              project_key = first_test.split("-")[0] if "-" in first_test else ""
              
              if not project_key:
                  print("Could not determine project key from test IDs")
                  exit(1)
              
              # Create a direct Jira issue creation payload
              jira_payload = {
                  "fields": {
                      "project": {
                          "key": project_key
                      },
                      "summary": test_info.get('summary', f"Test Execution {str(uuid.uuid4())[:8]}"),
                      "description": test_info.get('description', 'Automated test execution'),
                      "issuetype": {
                          "name": "Test Execution"
                      }
                  }
              }
              
              # Write the Jira issue creation payload
              with open('reports/xray-results/jira-execution.json', 'w') as f:
                  json.dump(jira_payload, f, indent=2)
              
              print("Successfully created Jira issue creation payload")
          except Exception as e:
              print(f"Error creating Jira payload: {str(e)}")
          EOF
          
          if [ -f "reports/xray-results/jira-execution.json" ]; then
            echo "Content of jira-execution.json:"
            cat reports/xray-results/jira-execution.json
            
            echo "\nCreating Test Execution in Jira..."
            if curl -s -o execution_response.json -w "%{http_code}" \
                 -H "Content-Type: application/json" \
                 -X POST \
                 -u "${{ secrets.JIRA_USERNAME }}:${{ secrets.JIRA_API_TOKEN }}" \
                 --data @reports/xray-results/jira-execution.json \
                 "${{ secrets.JIRA_URL }}/rest/api/2/issue" | grep -q "2[0-9][0-9]"; then
              echo "Successfully created Test Execution in Jira"
              cat execution_response.json
              
              # Extract the execution key
              EXECUTION_KEY=$(python -c "import json; print(json.load(open('execution_response.json'))['key'])")
              echo "Created Test Execution: $EXECUTION_KEY"
              
              # Now associate tests with the execution
              echo "\nAssociating tests with the execution..."
              for TEST_KEY in $(python -c "import json; print(' '.join([t['testKey'] for t in json.load(open('reports/xray-results/xray-report.json'))['tests']]))")
              do
                echo "Adding test $TEST_KEY to execution $EXECUTION_KEY"
                
                # Create a payload for the association
                echo "{\"add\": [\"$TEST_KEY\"]}" > test_add.json
                
                # Associate the test with the execution
                curl -s \
                     -H "Content-Type: application/json" \
                     -X POST \
                     -u "${{ secrets.JIRA_USERNAME }}:${{ secrets.JIRA_API_TOKEN }}" \
                     --data @test_add.json \
                     "${{ secrets.JIRA_URL }}/rest/raven/1.0/api/testexec/$EXECUTION_KEY/test"
              done
              
              echo "Successfully associated tests with execution"
              exit 0
            else
              echo "Failed to create Test Execution. Response:"
              cat execution_response.json || echo "No response"
            fi
          fi
          
          echo "\nAll attempts to upload to Xray failed. Please check your Xray setup and credentials."
          # Continue workflow even if Xray upload fails
        else
          echo "No Xray report found at reports/xray-results/xray-report.json"
        fi
